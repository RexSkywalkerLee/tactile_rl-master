{'params': {'seed': 3711, 'algo': {'name': 'a2c_continuous'}, 'model': {'name': 'continuous_a2c_logstd'}, 'network': {'name': 'actor_critic', 'separate': False, 'space': {'continuous': {'mu_activation': 'None', 'sigma_activation': 'None', 'mu_init': {'name': 'default'}, 'sigma_init': {'name': 'const_initializer', 'val': 0}, 'fixed_sigma': True}}, 'mlp': {'units': [512, 256, 256], 'activation': 'elu', 'd2rl': False, 'initializer': {'name': 'default'}, 'regularizer': {'name': 'None'}}}, 'load_checkpoint': False, 'load_path': '', 'config': {'name': 'Baseline', 'env_name': 'rlgpu', 'prefix': 'S1.0_C0.0_M0.02023-05-22_21-38-00-3895', 'user_prefix': '', 'auto_prefix': 'S1.0_C0.0_M0.0', 'multi_gpu': False, 'ppo': True, 'mixed_precision': False, 'normalize_input': True, 'normalize_value': True, 'value_bootstrap': True, 'num_actors': 8192, 'reward_shaper': {'scale_value': 0.01}, 'normalize_advantage': True, 'gamma': 0.99, 'tau': 0.95, 'learning_rate': 0.0001, 'lr_schedule': 'adaptive', 'schedule_type': 'standard', 'kl_threshold': 0.02, 'score_to_win': 100000, 'max_epochs': 20000, 'save_best_after': 100, 'save_frequency': 100, 'print_stats': True, 'grad_norm': 1.0, 'entropy_coef': 0.0, 'truncate_grads': 'Trues', 'e_clip': 0.2, 'horizon_length': 16, 'minibatch_size': 16384, 'mini_epochs': 4, 'critic_coef': 5, 'clip_value': True, 'seq_length': 4, 'bptt_len': 16, 'bounds_loss_coef': 0.005, 'weight_decay': 0.0, 'player_collect': False, 'player': {'deterministic': True, 'games_num': 2, 'games_repeat': 1, 'desired_games': 10000, 'print_stats': True}, 'central_value_config': {'minibatch_size': 16384, 'mini_epochs': 4, 'learning_rate': 0.0005, 'lr_schedule': 'adaptive', 'schedule_type': 'standard', 'kl_threshold': 0.016, 'clip_value': True, 'normalize_input': True, 'truncate_grads': True, 'network': {'name': 'actor_critic', 'central_value': True, 'mlp': {'units': [512, 256, 256], 'activation': 'elu', 'd2rl': False, 'initializer': {'name': 'default'}, 'regularizer': {'name': 'None'}}}}}}}
self.seed = 3711
Started to train
Adding Central Value Network
Get Override {}
CFG DICT {'name': 'AllegroArmMOAR', 'physics_engine': 'physx', 'env': {'rewardType': 'finger', 'sensor': 'thick', 'objInit': 'new', 'objSet': '27', 'skill_step': 500, 'spin_coef': 1.0, 'main_coef': 0.0, 'aux_coef': 0.0, 'vel_coef': -0.1, 'contact_coef': 0.0, 'torque_coef': -0.0003, 'work_coef': -0.0003, 'finger_coef': 0.1, 'handInit': 'default', 'numEnvs': 8192, 'envSpacing': 0.75, 'episodeLength': 500, 'enableDebugVis': False, 'aggregateMode': 1, 'sensorThresh': 1.0, 'sensorNoise': 0.1, 'obs_stack': 4, 'latency': 0.2, 'm_low': 0.2, 'm_up': 0.6, 'useInitRandomRotation': False, 'force_debug': False, 'numTestEnvs': 16, 'test': {'test_m_low': 0.1, 'test_m_up': 1.4}, 'robotStiffness': 3.0, 'clipObservations': 5.0, 'clipActions': 1.0, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'relScale': 0.2, 'useRelativeControl': True, 'usePrevTarget': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 0.8, 'controlFrequencyInv': 6, 'startPositionNoise': 0.01, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.01, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.2, 'resetDofVelRandomInterval': 0.0, 'forceScale': 2.0, 'forceProbRange': [0.2, 0.25], 'forceDecay': 0.99, 'forceDecayInterval': 0.1, 'disableSet': 0, 'axis': 'z', 'distRewardScale': -3.0, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': 0.0, 'controlPenaltyScale': -0.0, 'reachGoalBonus': 250, 'fallDistance': 0.1, 'fallPenalty': -50.0, 'objectType': 'block', 'observationType': 'partial_stack', 'asymmetric_observations': True, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetFileName': 'urdf/xarm6/xarm6_allegro_left_fsr.urdf', 'assetFileNameBlock': 'urdf/objects/cube_multicolor_allegro.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': True, 'randomization_params': {'frequency': 1000, 'observations': {'range': [0, 0.05], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian'}, 'actions': {'range': [0.0, 0.04], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian'}, 'sim_params': {'gravity': {'range': [0, 0.3], 'operation': 'additive', 'distribution': 'gaussian'}}, 'actor_params': {'hand': {'color': True, 'dof_properties': {'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform'}, 'lower': {'range': [0, 1e-05], 'operation': 'additive', 'distribution': 'gaussian'}, 'upper': {'range': [0, 1e-05], 'operation': 'additive', 'distribution': 'gaussian'}}}}}, 'enableCameraSensors': False}, 'sim': {'dt': 0.01667, 'substeps': 2, 'up_axis': 'z', 'use_gpu_pipeline': True, 'gravity': [0.0, 0.0, -9.81], 'physx': {'num_threads': 4, 'solver_type': 1, 'use_gpu': True, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'max_gpu_contact_pairs': 8388608, 'num_subscenes': 4, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 100.0, 'default_buffer_size_multiplier': 5.0, 'contact_collection': 2}}}
-0.0003 -0.0003
Obs type: partial_stack
[93m[1m[Warning] [carb.gym.plugin] useGpu is set, forcing single scene (0 subscenes)
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
ENTER ASSET CREATING!
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
ORB_COUNT 1
/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/leek/tactile_rl-master/isaacgymenvs/tasks/allegro_arm_morb_axis.py:678: DeprecationWarning: an integer is required (got type isaacgym._bindings.linux-x86_64.gym_38.DofDriveMode).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  asset_options.default_dof_drive_mode = gymapi.DOF_MODE_EFFORT
Num dofs:  22
Max effort:  50.0
Max effort:  50.0
Max effort:  32.0
Max effort:  32.0
Max effort:  32.0
Max effort:  20.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
Max effort:  10.0
DOF_LOWER_LIMITS [0.0, 0.782, -1.42391, 3.2866, 2.459, -1.48221, -0.47, -0.196, -0.174, -0.227, 0.7, 0.3, -0.189, -0.162, -0.47, -0.196, -0.174, -0.227, -0.47, -0.196, -0.174, -0.227]
DOF_UPPER_LIMITS [1e-05, 0.7820001, -1.4239, 3.28661, 2.4591, -1.4822, 0.47, 1.61, 1.709, 1.618, 1.396, 1.163, 1.644, 1.719, 0.47, 1.61, 1.709, 1.618, 0.47, 1.61, 1.709, 1.618]
PALM tensor(46, device='cuda:0') tensor([11, 13, 16, 29, 31, 34, 43, 39, 42, 21, 23, 25, 17, 35, 44, 26],
       device='cuda:0')
[93m[1m[Warning] [carb.gym.plugin] Acquiring DOF force tensor, but no actors have DOF force sensors enabled.
[93m[1m[Warning] [carb.gym.plugin] -> Enabled DOF force sensors for all eligible actors.
Contact Tensor Dimension torch.Size([8192, 147])
Num dofs:  22
Hand QPos Overriding: Idx:6 QPos: 0.0
Hand QPos Overriding: Idx:7 QPos: 0.0
Hand QPos Overriding: Idx:8 QPos: 0.0
Hand QPos Overriding: Idx:9 QPos: 0.0
Hand QPos Overriding: Idx:10 QPos: 1.3815
Hand QPos Overriding: Idx:11 QPos: 0.0868
Hand QPos Overriding: Idx:12 QPos: 0.1259
Hand QPos Overriding: Idx:13 QPos: 0.0
Hand QPos Overriding: Idx:14 QPos: 0.0048
Hand QPos Overriding: Idx:15 QPos: 0.0
Hand QPos Overriding: Idx:16 QPos: 0.0
Hand QPos Overriding: Idx:17 QPos: 0.0
Hand QPos Overriding: Idx:18 QPos: 0.0
Hand QPos Overriding: Idx:19 QPos: 0.0
Hand QPos Overriding: Idx:20 QPos: 0.0
Hand QPos Overriding: Idx:21 QPos: 0.0
Box(-1.0, 1.0, (22,), float32) Box(-inf, inf, (340,), float32) Box(-inf, inf, (217,), float32)
current training device: cuda:0
build mlp: 340
RunningMeanStd:  (1,)
RunningMeanStd:  (340,)
build mlp: 217
RunningMeanStd:  (1,)
RunningMeanStd:  (217,)
/home/leek/tactile_rl-master/isaacgymenvs/tasks/allegro_arm_morb_axis.py:1804: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  env_ids_torch = torch.tensor(env_ids,  device=self.device).long()
fps step: 8690 fps step and policy inference: 8421 fps total: 8293 epoch: 1/20000 frames: 0
fps step: 16774 fps step and policy inference: 16672 fps total: 16169 epoch: 2/20000 frames: 131072
Traceback (most recent call last):
  File "isaacgymenvs/train.py", line 193, in <module>
    launch_rlg_hydra()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "isaacgymenvs/train.py", line 182, in launch_rlg_hydra
    runner.run({
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/torch_runner.py", line 121, in run
    self.run_train(args)
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/torch_runner.py", line 102, in run_train
    agent.train()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/common/a2c_common.py", line 1226, in train
    step_time, play_time, update_time, sum_time, a_losses, c_losses, b_losses, entropies, kls, last_lr, lr_mul = self.train_epoch()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/common/a2c_common.py", line 1090, in train_epoch
    batch_dict = self.play_steps()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/common/a2c_common.py", line 674, in play_steps
    self.obs, rewards, self.dones, infos = self.env_step(res_dict['actions'])
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/common/a2c_common.py", line 495, in env_step
    obs, rewards, dones, infos = self.vec_env.step(actions)
  File "/home/leek/tactile_rl-master/isaacgymenvs/utils/rlgames_utils.py", line 156, in step
    return self.env.step(actions)
  File "/home/leek/tactile_rl-master/isaacgymenvs/tasks/base/vec_task.py", line 349, in step
    self.gym.simulate(self.sim)
KeyboardInterrupt
Traceback (most recent call last):
  File "isaacgymenvs/train.py", line 193, in <module>
    launch_rlg_hydra()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "isaacgymenvs/train.py", line 182, in launch_rlg_hydra
    runner.run({
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/torch_runner.py", line 121, in run
    self.run_train(args)
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/torch_runner.py", line 102, in run_train
    agent.train()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/common/a2c_common.py", line 1226, in train
    step_time, play_time, update_time, sum_time, a_losses, c_losses, b_losses, entropies, kls, last_lr, lr_mul = self.train_epoch()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/common/a2c_common.py", line 1090, in train_epoch
    batch_dict = self.play_steps()
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/common/a2c_common.py", line 674, in play_steps
    self.obs, rewards, self.dones, infos = self.env_step(res_dict['actions'])
  File "/home/leek/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/rl_games/common/a2c_common.py", line 495, in env_step
    obs, rewards, dones, infos = self.vec_env.step(actions)
  File "/home/leek/tactile_rl-master/isaacgymenvs/utils/rlgames_utils.py", line 156, in step
    return self.env.step(actions)
  File "/home/leek/tactile_rl-master/isaacgymenvs/tasks/base/vec_task.py", line 349, in step
    self.gym.simulate(self.sim)
KeyboardInterrupt